{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6YEXuY8HgX+lnSE8HmMU2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvuPs_9aF6Li",
        "outputId": "a3a507c1-0ec7-472d-b8f5-f27736987280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Multi-layer Perceptron (MLP) & Backpropagation Demonstration ---\n",
            "Initial W1:\n",
            "[[-0.083   0.2203]\n",
            " [-0.4999 -0.1977]]\n",
            "Initial W2:\n",
            "[[-0.3532]\n",
            " [-0.4077]]\n",
            "\n",
            "[STEP 1: FORWARD PASS]\n",
            "Hidden Layer Output (A1): [[0.3776 0.4507]]\n",
            "Final Prediction (y_pred): [[0.4214]]\n",
            "Initial MSE Loss: 0.3348\n",
            "\n",
            "[STEP 2: BACKWARD PASS (Gradient Calculation)]\n",
            "dLoss/dZ2: [[-0.1411]]\n",
            "dLoss/dW2:\n",
            "[[-0.0533]\n",
            " [-0.0636]]\n",
            "dLoss/dZ1: [[0.0117 0.0142]]\n",
            "dLoss/dW1:\n",
            "[[0.     0.    ]\n",
            " [0.0117 0.0142]]\n",
            "\n",
            "[STEP 3: WEIGHT UPDATE]\n",
            "Change in W1 (dLoss/dW1 * LR):\n",
            "[[0.     0.    ]\n",
            " [0.0012 0.0014]]\n",
            "New W1:\n",
            "[[-0.083   0.2203]\n",
            " [-0.5011 -0.1991]]\n",
            "Change in W2 (dLoss/dW2 * LR):\n",
            "[[-0.0053]\n",
            " [-0.0064]]\n",
            "New W2:\n",
            "[[-0.3479]\n",
            " [-0.4013]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. Define Activation and Loss Functions ---\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    # If x is the input (pre-activation), calculate sigmoid(x) * (1 - sigmoid(x))\n",
        "    # If x is already the sigmoid output (a), use a * (1 - a)\n",
        "    return x * (1 - x)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "# --- 2. Initial Setup (Simplified 2-Layer Network) ---\n",
        "np.random.seed(1)\n",
        "# 2 inputs, 2 hidden neurons, 1 output neuron\n",
        "input_size = 2\n",
        "hidden_size = 2\n",
        "output_size = 1\n",
        "\n",
        "# Initial weights and biases\n",
        "W1 = np.random.uniform(low=-0.5, high=0.5, size=(input_size, hidden_size)) # (2, 2)\n",
        "B1 = np.zeros((1, hidden_size)) # (1, 2)\n",
        "W2 = np.random.uniform(low=-0.5, high=0.5, size=(hidden_size, output_size)) # (2, 1)\n",
        "B2 = np.zeros((1, output_size)) # (1, 1)\n",
        "\n",
        "# Sample Data (e.g., XOR example)\n",
        "X = np.array([[0, 1]]) # Single input sample\n",
        "y_true = np.array([[1]]) # Target output\n",
        "\n",
        "print(\"--- Multi-layer Perceptron (MLP) & Backpropagation Demonstration ---\")\n",
        "print(f\"Initial W1:\\n{W1.round(4)}\")\n",
        "print(f\"Initial W2:\\n{W2.round(4)}\")\n",
        "\n",
        "# --- 3. FORWARD PASS ---\n",
        "print(\"\\n[STEP 1: FORWARD PASS]\")\n",
        "# Hidden Layer\n",
        "Z1 = np.dot(X, W1) + B1 # Pre-activation\n",
        "A1 = sigmoid(Z1) # Activation (Output of Hidden Layer)\n",
        "print(f\"Hidden Layer Output (A1): {A1.round(4)}\")\n",
        "\n",
        "# Output Layer\n",
        "Z2 = np.dot(A1, W2) + B2 # Pre-activation\n",
        "y_pred = sigmoid(Z2) # Final Prediction\n",
        "print(f\"Final Prediction (y_pred): {y_pred.round(4)}\")\n",
        "loss = mse_loss(y_true, y_pred)\n",
        "print(f\"Initial MSE Loss: {loss.round(4)}\")\n",
        "\n",
        "# --- 4. BACKWARD PASS (The Core of Backpropagation) ---\n",
        "print(\"\\n[STEP 2: BACKWARD PASS (Gradient Calculation)]\")\n",
        "\n",
        "# A. Output Layer Error (dLoss/dZ2)\n",
        "# dLoss/dy_pred * dy_pred/dZ2\n",
        "# dLoss/dy_pred = -(y_true - y_pred) for MSE\n",
        "# dy_pred/dZ2 = sigmoid_derivative(y_pred)\n",
        "dLoss_dZ2 = (y_pred - y_true) * sigmoid_derivative(y_pred) # (1, 1)\n",
        "print(f\"dLoss/dZ2: {dLoss_dZ2.round(4)}\")\n",
        "\n",
        "# B. Output Layer Weights Gradient (dLoss/dW2)\n",
        "# dLoss/dW2 = A1.T * dLoss/dZ2\n",
        "dLoss_dW2 = np.dot(A1.T, dLoss_dZ2) # (2, 1)\n",
        "print(f\"dLoss/dW2:\\n{dLoss_dW2.round(4)}\")\n",
        "\n",
        "# C. Hidden Layer Error (dLoss/dZ1)\n",
        "# dLoss/dZ1 = (dLoss/dZ2 * W2.T) * dZ1/dA1\n",
        "dLoss_dA1 = np.dot(dLoss_dZ2, W2.T) # (1, 2)\n",
        "dLoss_dZ1 = dLoss_dA1 * sigmoid_derivative(A1) # (1, 2)\n",
        "print(f\"dLoss/dZ1: {dLoss_dZ1.round(4)}\")\n",
        "\n",
        "# D. Hidden Layer Weights Gradient (dLoss/dW1)\n",
        "# dLoss/dW1 = X.T * dLoss/dZ1\n",
        "dLoss_dW1 = np.dot(X.T, dLoss_dZ1) # (2, 2)\n",
        "print(f\"dLoss/dW1:\\n{dLoss_dW1.round(4)}\")\n",
        "\n",
        "# --- 5. WEIGHT UPDATE (Gradient Descent) ---\n",
        "print(\"\\n[STEP 3: WEIGHT UPDATE]\")\n",
        "learning_rate = 0.1\n",
        "\n",
        "W2_new = W2 - learning_rate * dLoss_dW2\n",
        "W1_new = W1 - learning_rate * dLoss_dW1\n",
        "\n",
        "print(f\"Change in W1 (dLoss/dW1 * LR):\\n{(dLoss_dW1 * learning_rate).round(4)}\")\n",
        "print(f\"New W1:\\n{W1_new.round(4)}\")\n",
        "print(f\"Change in W2 (dLoss/dW2 * LR):\\n{(dLoss_dW2 * learning_rate).round(4)}\")\n",
        "print(f\"New W2:\\n{W2_new.round(4)}\")"
      ]
    }
  ]
}